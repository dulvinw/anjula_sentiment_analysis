{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd"
   ],
   "id": "5f488e77d679ef71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Loughran-McDonald sentiments",
   "id": "63384da6a106a9a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_loughran_mcdonald_dictionary(filepath):\n",
    "    # Load each sheet as a dictionary \n",
    "    sentiments = {}\n",
    "    sheet_names = ['Negative', 'Positive', 'Uncertainty', 'Litigious', 'StrongModal', 'WeakModal', 'Constraining']\n",
    "    \n",
    "    for sheet in sheet_names:\n",
    "        words = pd.read_excel(filepath, sheet_name=sheet, header=None).squeeze().tolist()\n",
    "        sentiments[sheet] = set(word.lower() for word in words) \n",
    "    \n",
    "    return sentiments\n",
    "\n",
    "dictionary_path = 'LoughranMcDonald_SentimentWordLists_2018.xlsx'\n",
    "sentiment_dictionary = load_loughran_mcdonald_dictionary(dictionary_path)"
   ],
   "id": "fa99ea54a37b9778"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate sentiment scores based on tokens",
   "id": "f347b3ba3f5d17e5"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer, stop words, and spaCy model\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# All possible sentiment categories including modals and raw modal counts\n",
    "sentiment_categories = [\"negative\", \"positive\", \"uncertainty\", \"litigious\", \"constraining\",\n",
    "                        \"strong_modal_positive\", \"strong_modal_negative\", \"strong_modal_uncertainty\",\n",
    "                        \"strong_modal_litigious\", \"strong_modal_constraining\",\n",
    "                        \"weak_modal_positive\", \"weak_modal_negative\", \"weak_modal_uncertainty\",\n",
    "                        \"weak_modal_litigious\", \"weak_modal_constraining\",\n",
    "                        \"strong_modal_raw_count\", \"weak_modal_raw_count\"]\n",
    "\n",
    "input_dir = 'Tickers_Json'\n",
    "output_dir = 'Sentiment_CSVs2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# calculate sentiment scores based on tokens\n",
    "def calculate_sentiment(tokens, sentiment_words):\n",
    "    return sum(1 for token in tokens if token in sentiment_words)\n",
    "\n",
    "# \n",
    "def parse_dependencies(raw_text):\n",
    "    doc = nlp(raw_text)\n",
    "    syntactic_sentiments = defaultdict(int)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ in sentiment_dictionary[\"StrongModal\"]:\n",
    "            modal_type = \"strong_modal\"\n",
    "            syntactic_sentiments[\"strong_modal_raw_count\"] += 1\n",
    "        elif token.lemma_ in sentiment_dictionary[\"WeakModal\"]:\n",
    "            modal_type = \"weak_modal\"\n",
    "            syntactic_sentiments[\"weak_modal_raw_count\"] += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for child in token.children:\n",
    "            if child.dep_ in (\"dobj\", \"ccomp\", \"xcomp\", \"advcl\", \"acomp\", \"neg\", \"amod\", \"npadvmod\"):\n",
    "                action = child.lemma_\n",
    "\n",
    "                if action in sentiment_dictionary[\"Positive\"]:\n",
    "                    syntactic_sentiments[f\"{modal_type}_positive\"] += 1\n",
    "                elif action in sentiment_dictionary[\"Negative\"]:\n",
    "                    syntactic_sentiments[f\"{modal_type}_negative\"] += 1\n",
    "                elif action in sentiment_dictionary[\"Uncertainty\"]:\n",
    "                    syntactic_sentiments[f\"{modal_type}_uncertainty\"] += 1\n",
    "                elif action in sentiment_dictionary[\"Litigious\"]:\n",
    "                    syntactic_sentiments[f\"{modal_type}_litigious\"] += 1\n",
    "                elif action in sentiment_dictionary[\"Constraining\"]:\n",
    "                    syntactic_sentiments[f\"{modal_type}_constraining\"] += 1\n",
    "\n",
    "    return syntactic_sentiments\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        company_name = os.path.splitext(file_name)[0]\n",
    "        output_file = os.path.join(output_dir, f\"{company_name}_sentiments.csv\")\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        year_quarter_sentiments = defaultdict(lambda: defaultdict(lambda: {'sentiments': defaultdict(int), 'dates': []}))\n",
    "\n",
    "        for entry in data:\n",
    "            year = entry['year']\n",
    "            quarter = entry['quarter']\n",
    "            date = entry['time']\n",
    "            transcript = ' '.join([speech for segment in entry['transcript'] for speech in segment['speech']])\n",
    "\n",
    "            for category in sentiment_categories:\n",
    "                year_quarter_sentiments[year][quarter]['sentiments'][category] = 0\n",
    "\n",
    "            tokens = preprocess_text(transcript)\n",
    "\n",
    "            year_quarter_sentiments[year][quarter]['sentiments']['negative'] += calculate_sentiment(tokens, sentiment_dictionary[\"Negative\"])\n",
    "            year_quarter_sentiments[year][quarter]['sentiments']['positive'] += calculate_sentiment(tokens, sentiment_dictionary[\"Positive\"])\n",
    "            year_quarter_sentiments[year][quarter]['sentiments']['uncertainty'] += calculate_sentiment(tokens, sentiment_dictionary[\"Uncertainty\"])\n",
    "            year_quarter_sentiments[year][quarter]['sentiments']['litigious'] += calculate_sentiment(tokens, sentiment_dictionary[\"Litigious\"])\n",
    "            year_quarter_sentiments[year][quarter]['sentiments']['constraining'] += calculate_sentiment(tokens, sentiment_dictionary[\"Constraining\"])\n",
    "\n",
    "            syntactic_sentiments = parse_dependencies(transcript)\n",
    "            for sentiment, count in syntactic_sentiments.items():\n",
    "                year_quarter_sentiments[year][quarter]['sentiments'][sentiment] += count\n",
    "\n",
    "            year_quarter_sentiments[year][quarter]['dates'].append(date)\n",
    "\n",
    "        # Write sentiment data to CSV\n",
    "        with open(output_file, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            header = ['ID', 'Year', 'Quarter', 'Date'] + sentiment_categories\n",
    "            writer.writerow(header)\n",
    "\n",
    "            for year, quarters in year_quarter_sentiments.items():\n",
    "                for quarter, data in quarters.items():\n",
    "                    unique_id = f\"{year}_Q{quarter}\"\n",
    "                    earliest_date = min(data['dates'])  \n",
    "                    row = [unique_id, year, quarter, earliest_date]\n",
    "                    for category in sentiment_categories:\n",
    "                        row.append(data['sentiments'][category])\n",
    "                    writer.writerow(row)\n",
    "\n",
    "        print(f\"Sentiment data for {company_name} saved to {output_file}\")\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Overall Sentiment Score calculation",
   "id": "4aa6c09367330405"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_dir = 'Sentiment_CSVs2'\n",
    "output_dir = 'Sentioment_Scores2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Sentiment Score calculation functions\n",
    "def calculate_nss(row):\n",
    "    return (row['positive'] - row['negative']) / (row['positive'] + row['negative'] + 1e-6)  \n",
    "\n",
    "\n",
    "def calculate_wss(row):\n",
    "    weights = {\n",
    "        'positive': 1.0, 'negative': -1.0,\n",
    "        'strong_modal_positive': 1.5, 'strong_modal_negative': -1.5,\n",
    "        'weak_modal_positive': 0.5, 'weak_modal_negative': -0.5\n",
    "    }\n",
    "    score = 0.0\n",
    "    for sentiment, weight in weights.items():\n",
    "        score += row[sentiment] * weight\n",
    "    return score / (sum(abs(weight) for weight in weights.values()))\n",
    "\n",
    "\n",
    "def calculate_overall_sentiment(row):\n",
    "    weights = {\n",
    "        'positive': 1.0,\n",
    "        'negative': -1.0,\n",
    "        'uncertainty': -0.5,\n",
    "        'litigious': -0.5,\n",
    "        'constraining': -0.5,\n",
    "        'strong_modal_raw_count': -0.2,\n",
    "        'weak_modal_raw_count': -0.1\n",
    "    }\n",
    "    overall_score = sum(row[category] * weight for category, weight in weights.items())\n",
    "    return overall_score\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Calculate NSS, WSS, and Overall Sentiment Score\n",
    "        df['NSS'] = df.apply(calculate_nss, axis=1)\n",
    "        df['WSS'] = df.apply(calculate_wss, axis=1)\n",
    "        df['Overall_Sentiment_Score'] = df.apply(calculate_overall_sentiment, axis=1)\n",
    "\n",
    "\n",
    "        # Determine binary sentiment based on NSS\n",
    "        def binary_sentiment(nss):\n",
    "            if nss > 0.1:\n",
    "                return 'positive'\n",
    "            elif nss < -0.1:\n",
    "                return 'negative'\n",
    "            else:\n",
    "                return 'neutral'\n",
    "\n",
    "\n",
    "        df['binary_sentiment'] = df['NSS'].apply(binary_sentiment)\n",
    "\n",
    "        output_file = os.path.join(output_dir, file_name)\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Processed {file_name} and saved to {output_file}\")\n",
    "\n"
   ],
   "id": "1d0af5de91e9f5b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
